\documentclass[twocolumn, 10pt]{article} % 全局字体大小设为12pt

% 导入 ctex 包以支持中文
\usepackage[UTF8]{ctex}

% 设置页面边距
\usepackage{geometry}
\geometry{left=2cm,right=2cm,top=2.5cm,bottom=2.5cm}

% 其他常用包
\usepackage{graphicx}   % 插入图片
\usepackage{amsmath}    % 数学公式
\usepackage{amssymb}    % 数学符号
\usepackage{hyperref}   % 超链接
\usepackage{enumitem}   % 引入 enumitem 包用于自定义列表
\usepackage{array} % 提供表格增强功能
\usepackage{stfloats} % 支持双栏排版中的浮动对象


\usepackage{xcolor} % 用于设置文本和背景颜色
\usepackage{times} % 设置全局字体为 Times New Roman，或者根据需求选择其他字体
% 表格字体设置
\usepackage{etoolbox}

% 导入titlecaps宏包
\usepackage{titlecaps}




% 自定义命令将标题首字母大写，其他单词小写
\Addlcwords{a an the of and in on at to with by for from}
\newcommand{\capitalizeTitle}[1]{\titlecap{#1}}

% 设置标题格式
\usepackage{titlesec}
\titleformat{\section}
  {\normalfont\Large\bfseries}
  {\thesection}{1em}{\capitalizeTitle}

\titleformat{\subsection}
  {\normalfont\large\bfseries}
  {\thesubsection}{1em}{\capitalizeTitle}

\titleformat{\subsubsection}
  {\normalfont\normalsize\bfseries}
  {\thesubsubsection}{1em}{\capitalizeTitle}









\AtBeginEnvironment{tabular}{\small} % 将表格内字体设为比正文小1号


% 定义浅黄色
\definecolor{lightyellow}{rgb}{1.0, 1.0, 0.88}

\begin{document}

% 标题
\title{知识点总结}
\author{作者姓名}
\date{\today}
\maketitle
\begin{abstract}
    模型加速; 大模型; 语音; 图像; 多目标跟踪等。
\end{abstract}



\section{Asynchronous Methods for Deep Reinforcement Learning}
the most general
and successful reinforcement learning agent to date


这篇论文《Asynchronous Methods for Deep Reinforcement Learning》介绍了一种异步深度强化学习的方法，旨在通过\colorbox{lightyellow}{异步梯度下降优化深度神经网络控制器} (asynchronous variant of actor-critic)。这种方法的提出旨在解决深度强化学习中与神经网络结合时的不稳定性问题。 surpasses
the current state-of-the-art on the Atari domain
while training for half the time on a single
multi-core CPU instead of a GPU. 以下是对论文的详细解读：

\subsection{背景与动机}
\begin{itemize}
    \item \textbf{强化学习与神经网络的结合}：深度神经网络能够提供丰富的表示，这使得强化学习算法能够更有效地工作。然而，简单的在线强化学习算法与深度神经网络的结合往往会导致不稳定的训练过程。
    \item \textbf{经验回放（Experience Replay）的局限性}：经验回放是一种常用的技术，通过将智能体的数据存储在回放记忆中进行批量处理或随机采样，以减少数据的非平稳性，并去相关更新。但这种方法需要大量的内存和计算资源，并且只能用于离策略学习算法。
\end{itemize}

\subsection{主要贡献}
\begin{itemize}
    \item \textbf{异步并行的提出}：论文提出了一种不使用经验回放的替代方法，即异步并行执行多个智能体。多个智能体在不同的环境实例中并行执行，从而将数据分散到一个更平稳的过程。这种简单的思想使得许多基本的在线强化学习算法（如Sarsa、n步方法和演员-评论家方法）可以稳健有效地与深度神经网络结合使用。
\end{itemize}


\subsection{ 异步深度强化学习框架}
\begin{itemize}
    \item \textbf{异步一阶Q学习}：使用多线程异步执行的Q学习，每个线程与其环境副本交互，并在每个步骤计算Q学习损失的梯度。采用一个共享且缓慢变化的目标网络计算Q学习损失，并通过累积多个时间步的梯度来减少多线程间更新的冲突。
\end{itemize}
\begin{itemize}
    \item \textbf{异步n步Q学习}：这种方法使用n步回报来加速奖励的传播，并通过前向视图进行n步回报的计算。每个线程在达到终止状态或执行了最大步数后，计算梯度并执行异步更新。
\end{itemize}
\begin{itemize}
    \item \textbf{异步优势演员-评论家（A3C)}：在A3C方法中，智能体维护一个策略网络和一个价值函数估计。每个线程在每次执行最大步数或到达终止状态后，计算并更新策略和价值函数的梯度。
\end{itemize}

\subsection{实验与结果}
\begin{itemize}
    \item \textbf{Atari 2600游戏实验}：在多个Atari游戏上，A3C方法和其他异步方法与DQN进行比较。实验结果显示，异步方法比DQN在学习速度上有显著提升，特别是在16核CPU上运行时，这些方法的训练速度大大快于使用GPU的DQN。
\end{itemize}
\begin{itemize}
    \item \textbf{连续动作控制任务}：在MuJoCo物理模拟器中的连续动作任务上，A3C方法成功地学习到了良好的策略，表明该方法能够很好地扩展到连续动作空间。
\end{itemize}
\begin{itemize}
    \item \textbf{Labyrinth 3D迷宫任务}：在Labyrinth任务中，智能体必须学习在随机生成的迷宫中找到奖励。实验表明，A3C方法能够成功学习到适应新迷宫的策略。
\end{itemize}

\subsection{ 结论与讨论}
\begin{itemize}
    \item \textbf{稳定性与鲁棒性}：实验表明，异步并行的方式对多种强化学习算法的稳定性有显著的改善。使用多线程并行执行并更新共享模型，不仅减少了训练的不稳定性，还大大提高了训练速度。
\end{itemize}
\begin{itemize}
    \item \textbf{经验回放的替代}：异步并行提供了传统经验回放之外的另一种方式来稳定训练过程，并且不需要额外的内存或计算资源。
\end{itemize}

\subsection{总结}
论文通过提出异步并行方法，展示了在单机多线程环境下如何有效地进行深度强化学习。A3C作为其中最成功的方法，表现出了极高的通用性，不仅在2D和3D游戏中表现良好，还能适用于离散和连续动作空间。这一方法的提出大大推进了深度强化学习在各种复杂任务中的应用。


\section{Continuous Control with Deep Reinforcement Learning}
这篇论文《Continuous Control with Deep Reinforcement Learning》是由Google DeepMind的研究人员在2016年ICLR会议上发表的，提出了一种用于连续动作空间的深度强化学习算法，称为深度确定性策略梯度（Deep Deterministic Policy Gradient, DDPG）。以下是对论文的详细解读：

\subsection{背景与动机}
\begin{itemize}
    \item \textbf{强化学习与连续动作空间的挑战}：传统的强化学习方法（如Q学习）在处理离散动作空间时效果较好，但面对连续动作空间时却存在计算上的困难。这是因为在连续动作空间中，需要进行全局优化来选择每个时间步的最佳动作，这对于复杂的函数逼近器（如神经网络）来说非常慢且不切实际。
\end{itemize}
\begin{itemize}
    \item \textbf{深度Q网络（DQN）的局限性}：DQN在处理高维观察空间时表现出色（如在Atari游戏中），但它只适用于离散动作空间。简单地将动作空间离散化虽然可行，但会带来维度灾难的问题，使得训练非常困难。
\end{itemize}


\subsection{DDPG算法的提出}
\begin{itemize}
    \item \textbf{确定性策略梯度}：DDPG基于确定性策略梯度（Deterministic Policy Gradient, DPG）算法，它维护一个参数化的策略函数$\mu  \left( s|\theta^{\mu} \right)$
，该函数将状态映射到具体的动作。这一策略函数是确定性的，而非像传统策略梯度方法中的随机策略。
\end{itemize}
\begin{itemize}
    \item \textbf{结合DQN的优势}：DDPG结合了DQN中的两个关键技巧：经验回放（replay buffer）和目标网络（target network），这两者极大地改善了训练的稳定性。
\begin{itemize}
    \item \textbf{经验回放}：通过存储智能体在环境中探索的经验并进行随机抽样，可以打破样本间的时间相关性，使训练过程更加稳定。
    \item \textbf{目标网络}：通过创建一个慢速更新的目标网络，减少Q学习中因目标值和Q值都依赖于同一网络参数而导致的不稳定性。
\end{itemize}
\end{itemize}


\subsection{算法细节}
\begin{itemize}
    \item \textbf{Critic更新}：使用贝尔曼方程来更新Q值网络，公式如下：
\begin{align}
L\left( \theta^{Q} \right) = 
\end{align}
\end{itemize}
\begin{itemize}
    \item \textbf{Actor更新}：Actor的更新基于策略梯度，使用Critic对当前策略的梯度进行估计，公式如下：
\end{itemize}


\section{ GPT系列-Improving Language Understanding by Generative Pre-Training, gpt1, 2018}

\section{GPT系列-Language Models are Unsupervised Multitask Learners, gpt2, 2019}

\section{GPT系列-Language Models are Few-Shot Learners, gpt3, 2020}


\section{DALL·E系列Zero-Shot Text-to-Image Generation, DALLE, 2021}

\section{DALL·E系列Hierarchical Text-Conditional Image Generation with CLIP Latents, DALLE2, 2022}

\section{CLIP-Learning Transferable Visual Models From Natural Language Supervision, 2021}


\section{Reinforcement Learning系列-Playing Atari with Deep Reinforcement Learning, 2013}


\section{Reinforcement Learning系列-Learning Dexterous In-Hand Manipulation, 2018}


\section{Reinforcement Learning系列-OpenAI Five, 2019}


\section{安全性与对齐-Aligning Superhuman AI with Human Intentions: A Bayesian Approach" (Irving et al, 2019}

\section{安全性与对齐-AI Safety via Debate, 2019}



\section{AI Policy与影响力-AI and Compute" (Amodei  Hernandez, 2018)}


























\end{document}
